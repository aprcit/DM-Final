{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66295d55-c399-4a5c-b442-98afc5b6c0d3",
   "metadata": {},
   "source": [
    "# 加载数据 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "679e52c3-5e98-4c45-9319-aa025a65412f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# %load load_data.py\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtext_analyse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m text_embedding\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrow_normalize\u001b[39m(mx):\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Row-normalize matrix\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/DataMining/期末大作业/text_analyse.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer, TfidfTransformer\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# %load load_data.py\n",
    "import numpy as np\n",
    "from text_analyse import text_embedding\n",
    "\n",
    "def row_normalize(mx):\n",
    "    \"\"\"Row-normalize matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))                # 每一行求和\n",
    "    r_inv = np.power(rowsum, -0.5).flatten()    # 返回一个一维数组\n",
    "    r_inv[np.isinf(r_inv)] = 0.                 # 一维数组中，如果有inf，变为0\n",
    "    r_mat_inv = np.eye(r_inv.shape[0])\n",
    "    for i in range(r_inv.shape[0]):\n",
    "        r_mat_inv[i][i] = r_inv[i]\n",
    "    mx = mx.dot(r_mat_inv).transpose().dot(r_mat_inv)   # 行归一化\n",
    "    return mx\n",
    "\n",
    "\n",
    "def load_data_1(dataset='cora'):\n",
    "    path = r'data_1\\{}'.format(dataset)\n",
    "\n",
    "    feature = []\n",
    "    with open(path+'.feature', 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            # print(line)\n",
    "            feature.append([int(k) for k in line.strip().split()])\n",
    "            # print(feature)\n",
    "            # break\n",
    "    feature_arr = np.array(feature)\n",
    "    print(feature_arr.shape)\n",
    "\n",
    "    label = []\n",
    "    with open(path+'.label', 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            # print(line)\n",
    "            label.append([int(line.strip())])\n",
    "            # print(feature)\n",
    "            # break\n",
    "    label_arr = np.array(label)\n",
    "    print(label_arr.shape)\n",
    "\n",
    "    adj = np.zeros((label_arr.shape[0], label_arr.shape[0]), dtype=np.int8)\n",
    "    with open(path+'.edge', 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            nodes = [int(k) for k in line.strip().split()]\n",
    "            # print(nodes)\n",
    "            adj[nodes[0], nodes[1]] = 1\n",
    "    print(adj, adj.shape)\n",
    "\n",
    "    return feature_arr, label_arr, adj\n",
    "\n",
    "\n",
    "def load_data_2(dataset='cora'):\n",
    "    path = r'data_2\\{}'.format(dataset)\n",
    "\n",
    "    text = []\n",
    "    with open(path+'.text', 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            text.append(line.strip().split(maxsplit=1)[1])\n",
    "    feature_arr = text_embedding(text)\n",
    "    print(feature_arr.shape)\n",
    "    # exit(0)\n",
    "\n",
    "    label = []\n",
    "    with open(path+'.label', 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            # print(line)\n",
    "            label.append([int(line.strip().split()[1])])\n",
    "            # print(feature)\n",
    "            # break\n",
    "    label_arr = np.array(label)\n",
    "    print(label_arr.shape)\n",
    "\n",
    "    adj = np.zeros((label_arr.shape[0], label_arr.shape[0]), dtype=np.int8)\n",
    "    with open(path+'.edge', 'r', encoding='utf-8') as f:\n",
    "        for line in f.readlines():\n",
    "            nodes = [int(k) for k in line.strip().split()]\n",
    "            # print(nodes)\n",
    "            adj[nodes[0], nodes[1]] = 1\n",
    "    print(adj, adj.shape)\n",
    "\n",
    "    return feature_arr, label_arr, adj\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # load_data_1()\n",
    "    load_data_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf41dba-730f-4c11-96c3-44d25f36dab4",
   "metadata": {},
   "source": [
    "# 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ecce8ee-3a3f-47a8-9ca5-07dbbb667d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load model.py\n",
    "from collections import OrderedDict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size, out_size, num_layers=3, dropout=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.layer_1 = nn.Linear(in_size, hidden_size, bias=True)\n",
    "        self.layer_2 = nn.Linear(hidden_size, out_size, bias=True)\n",
    "\n",
    "        '''\n",
    "        if num_layers == 1:\n",
    "            hidden_size = out_size\n",
    "\n",
    "        self.pipeline = nn.Sequential(OrderedDict([\n",
    "            ('layer_0', nn.Linear(in_size, hidden_size, bias=(num_layers != 1))),\n",
    "            ('dropout_0', nn.Dropout(dropout)),\n",
    "            ('relu_0', nn.ReLU())\n",
    "        ]))\n",
    "\n",
    "        for i in range(1, num_layers):\n",
    "            if i == num_layers - 1:\n",
    "                self.pipeline.add_module('layer_{}'.format(i), nn.Linear(hidden_size, out_size, bias=True))\n",
    "            else:\n",
    "                self.pipeline.add_module('layer_{}'.format(i), nn.Linear(hidden_size, hidden_size, bias=True))\n",
    "                self.pipeline.add_module('dropout_{}'.format(i), nn.Dropout(dropout))\n",
    "                self.pipeline.add_module('relu_{}'.format(i), nn.ReLU())\n",
    "        '''\n",
    "\n",
    "        self.weights_init()\n",
    "\n",
    "    def weights_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, feature):\n",
    "        '''\n",
    "        return F.softmax(self.pipeline(feature), dim=1)\n",
    "        '''\n",
    "        h = F.relu(self.layer_1(feature))\n",
    "        out = self.layer_2(h)\n",
    "        return out\n",
    "\n",
    "\n",
    "class GNN(nn.Module):\n",
    "    def __init__(self, in_size, hidden_size, out_size):\n",
    "        super(GNN, self).__init__()\n",
    "        self.layer_1 = nn.Linear(in_size, hidden_size, bias=True)\n",
    "        self.layer_2 = nn.Linear(hidden_size, out_size, bias=True)\n",
    "\n",
    "    def weights_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, x, adj):\n",
    "        x_agg = torch.mm(adj, x)\n",
    "        h = F.relu(self.layer_1(x_agg))\n",
    "\n",
    "        h_agg = torch.mm(adj, h)\n",
    "        out = self.layer_2(h_agg)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c058ffd-ab46-459d-9f88-1bc9361394f7",
   "metadata": {},
   "source": [
    "# 文本分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "304b974f-19e5-49bf-a8f9-553434a6cc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load text_analyse.py\n",
    "import warnings\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "params_count = {\n",
    "    'analyzer': 'word',     # 取值'word'-分词结果为词级、'char'-字符级(结果会出现he is，空格在中间的情况)、'char_wb'-字符级(以单词为边界)，默认值为'word'\n",
    "    'binary': False,        # boolean类型，设置为True，则所有非零计数都设置为1.（即，tf的值只有0和1，表示出现和不出现）\n",
    "    'decode_error': 'strict',\n",
    "    'dtype': np.float64,    # 输出矩阵的数值类型\n",
    "    'encoding': 'utf-8',\n",
    "    'input': 'content',     # 取值filename，文本内容所在的文件名；file，序列项必须有一个'read'方法，被调用来获取内存中的字节；content，直接输入文本字符串\n",
    "    'lowercase': True,      # boolean类型，计算之前是否将所有字符转换为小写。\n",
    "    'max_df': 1.0,          # 词汇表中忽略文档频率高于该值的词；取值在[0,1]之间的小数时表示文档频率的阈值，取值为整数时(>1)表示文档频数的阈值；如果设置了vocabulary，则忽略此参数。\n",
    "    'min_df': 1,            # 词汇表中忽略文档频率低于该值的词；取值在[0,1]之间的小数时表示文档频率的阈值，取值为整数时(>1)表示文档频数的阈值；如果设置了vocabulary，则忽略此参数。\n",
    "    'max_features': None,   # int或None(默认值).设置int值时建立一个词汇表，仅用词频排序的前max_features个词创建语料库；如果设置了vocabulary，则忽略此参数。\n",
    "    'ngram_range': (1, 2),  # 要提取的n-grams中n值范围的下限和上限，min_n <= n <= max_n。\n",
    "    'preprocessor': None,   # 覆盖预处理（字符串转换）阶段，同时保留标记化和 n-gram 生成步骤。仅适用于analyzer不可调用的情况。\n",
    "    'stop_words': 'english',    # 仅适用于analyzer='word'。取值english，使用内置的英语停用词表；list，自行设置停停用词列表；默认值None，不会处理停用词\n",
    "    'strip_accents': None,\n",
    "    'token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',   # 分词方式、正则表达式，默认筛选长度>=2的字母和数字混合字符（标点符号被当作分隔符）。仅在analyzer='word'时使用。\n",
    "    'tokenizer': None,      # 覆盖字符串标记化步骤，同时保留预处理和 n-gram 生成步骤。仅适用于analyzer='word'\n",
    "    'vocabulary': None,     # 自行设置词汇表（可设置字典），如果没有给出，则从输入文件/文本中确定词汇表\n",
    "}\n",
    "params_tfidf = {\n",
    "    'norm': 'l2',           # 输出结果是否标准化/归一化。l2：向量元素的平方和为1，当应用l2范数时，两个向量之间的余弦相似度是它们的点积；l1：向量元素的绝对值之和为1\n",
    "    'smooth_idf': True,     # 在文档频率上加1来平滑 idf ，避免分母为0\n",
    "    'sublinear_tf': True,  # 应用次线性 tf 缩放，即将 tf 替换为 1 + log(tf)\n",
    "    'use_idf': True,        # 是否计算idf，布尔值，False时idf=1。\n",
    "}\n",
    "\n",
    "\n",
    "def text_embedding(data):\n",
    "    '''\n",
    "    class LemmaTokenizer:\n",
    "        def __init__(self):\n",
    "            self.wnl = WordNetLemmatizer()\n",
    "\n",
    "        def __call__(self, doc):\n",
    "            words = []\n",
    "            for t in word_tokenize(doc):\n",
    "                if len(t) < 3 or \"'\" in t or \"~\" in t:\n",
    "                    continue\n",
    "                words.append(self.wnl.lemmatize(t))\n",
    "            return words\n",
    "\n",
    "    params_count['tokenizer'] = LemmaTokenizer()\n",
    "    '''\n",
    "\n",
    "    params_count['max_features'] = 500\n",
    "    params_count['max_df'] = 0.8\n",
    "    params_count['min_df'] = 0.01\n",
    "\n",
    "    cv = CountVectorizer(**params_count)\n",
    "    x_cv = cv.fit_transform(data)\n",
    "\n",
    "    vocabulary = cv.get_feature_names_out()\n",
    "    print(vocabulary)\n",
    "    # print(x_cv.toarray())\n",
    "\n",
    "    tt = TfidfTransformer(**params_tfidf)\n",
    "    x_tfidf = tt.fit_transform(x_cv.toarray())\n",
    "    # print(x_tfidf.toarray())\n",
    "\n",
    "    return x_tfidf.toarray()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_data = [\"Chinese Beijing Chinese\",\n",
    "                  \"Chinese Chinese Shanghai\",\n",
    "                  \"Chinese Macao\",\n",
    "                  \"Tokyo Japan Chinese\"]\n",
    "    text_embedding(train_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9265459-9943-4af0-b6d1-ea46c2bf7c05",
   "metadata": {},
   "source": [
    "# gnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2e90259-5ce9-4ebb-9a2e-4c7049de6a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load train_gnn.py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from load_data import load_data_1, load_data_2, row_normalize\n",
    "from model import GNN\n",
    "\n",
    "dataset = 'cora'\n",
    "feature, label, adj = load_data_2(dataset)\n",
    "num_classes = len(np.unique(label))\n",
    "adj = row_normalize(adj)\n",
    "\n",
    "idx_train, idx_test, _, _ = train_test_split(\n",
    "    torch.LongTensor(np.arange(label.shape[0])), label, test_size=0.4, random_state=2333)\n",
    "print(idx_train, idx_test)\n",
    "\n",
    "adj = torch.FloatTensor(adj)\n",
    "feature = torch.FloatTensor(feature)\n",
    "label = torch.LongTensor(label).flatten()\n",
    "\n",
    "model = GNN(in_size=feature.shape[1], hidden_size=64, out_size=num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(feature, adj)\n",
    "    loss_train = F.cross_entropy(output[idx_train], label[idx_train])\n",
    "\n",
    "    _, output = torch.max(output, dim=1)\n",
    "    acc_train = accuracy_score(label[idx_train].detach().numpy(), output[idx_train])\n",
    "\n",
    "    print('epoch:{:3d}: | loss:{:1.5f} | acc:{:.3f}'.format(epoch, loss_train, acc_train))\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "output = model(feature, adj)\n",
    "\n",
    "_, output = torch.max(output, dim=1)\n",
    "acc_test = accuracy_score(label[idx_test].detach().numpy(), output[idx_test])\n",
    "print(acc_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7a62c8-2255-4c94-bba5-9ae34efd4e38",
   "metadata": {},
   "source": [
    "# mlp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f12b3775-f69f-42c1-b4d4-092865b81505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load train_mlp.py\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from load_data import load_data_1, load_data_2\n",
    "from model import MLP\n",
    "\n",
    "dataset = 'cora'\n",
    "feature, label, _ = load_data_2(dataset)\n",
    "num_classes = len(np.unique(label))\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(feature, label, test_size=0.4, random_state=2333)\n",
    "print(y_train.shape, y_test.shape)\n",
    "y_train = y_train.flatten()\n",
    "y_test = y_test.flatten()\n",
    "\n",
    "x_train, x_test = [torch.FloatTensor(k) for k in [x_train, x_test]]\n",
    "y_train, y_test = [torch.LongTensor(k) for k in [y_train, y_test]]\n",
    "print(y_train, y_test)\n",
    "\n",
    "model = MLP(in_size=x_train.shape[1], hidden_size=64, out_size=num_classes)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    output = model(x_train)\n",
    "    loss_train = F.cross_entropy(output, y_train)\n",
    "\n",
    "    _, output = torch.max(output, dim=1)\n",
    "    acc_train = accuracy_score(y_train.detach().numpy(), output)\n",
    "\n",
    "    print('epoch:{:3d}: | loss:{:1.5f} | acc:{:.3f}'.format(epoch, loss_train, acc_train))\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()\n",
    "output = model(x_test)\n",
    "\n",
    "_, output = torch.max(output, dim=1)\n",
    "acc_test = accuracy_score(y_test.detach().numpy(), output)\n",
    "print(acc_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
